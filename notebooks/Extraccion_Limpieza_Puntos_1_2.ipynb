{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c82c90d",
   "metadata": {},
   "source": [
    "# Predicciones Meteorológicas (AEMET) - SPRINT I\n",
    "\n",
    "Parte 1 - Extracción de Datos\n",
    "\n",
    "Navegar la documentación de la API de AEMET y explorar los endpoints\n",
    "\n",
    "Desarrollar un script que extraiga la información histórica de todas las provincias.\n",
    "\n",
    "Ejecutar el script para extraer los datos de los últimos dos años y verificar que todo funcione correctamente.\n",
    "\n",
    "En el modelo de datos, cada registro debe tener un timestamp de extracción y un identificador para que se pueda manejar el sistema de actualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"AEMET_API_KEY\")\n",
    "CSV_ESTACIONES = \"data/estaciones_filtradas.csv\"\n",
    "ARCHIVO_SALIDA = \"data/temperaturas_historicas_ampliadas.csv\"\n",
    "\n",
    "# 4 rangos de fechas para cubrir 2 años\n",
    "FECHAS = [\n",
    "    (\"2023-05-29T00:00:00UTC\", \"2023-11-28T00:00:00UTC\"),\n",
    "    (\"2023-11-29T00:00:00UTC\", \"2024-05-28T00:00:00UTC\"),\n",
    "    (\"2024-05-29T00:00:00UTC\", \"2024-11-28T00:00:00UTC\"),\n",
    "    (\"2024-11-29T00:00:00UTC\", \"2025-05-28T00:00:00UTC\")\n",
    "]\n",
    "\n",
    "# Cargamos los idema descargados\n",
    "if os.path.exists(ARCHIVO_SALIDA):\n",
    "    datos_existentes = pd.read_csv(ARCHIVO_SALIDA)\n",
    "    estaciones_descargadas = set(datos_existentes[\"idema\"].unique())\n",
    "else:\n",
    "    estaciones_descargadas = set()\n",
    "\n",
    "# Estaciones\n",
    "estaciones = pd.read_csv(CSV_ESTACIONES)\n",
    "\n",
    "# Lista para guardar todos los datos\n",
    "todas_las_filas = []\n",
    "\n",
    "# Creamos identificador único para esta descarga\n",
    "id_descarga = str(uuid.uuid4())\n",
    "\n",
    "print(\"Cargando estaciones...\")\n",
    "\n",
    "for _, fila in estaciones.iterrows():\n",
    "    codigo = fila[\"indicativo\"]\n",
    "    nombre = fila[\"nombre\"]\n",
    "\n",
    "    if codigo in estaciones_descargadas:\n",
    "        continue\n",
    "\n",
    "    print(f\"Procesando estación: {codigo} - {nombre}\")\n",
    "\n",
    "    datos_estacion = []\n",
    "    for fecha_inicio, fecha_fin in FECHAS:\n",
    "        url_meta = (\n",
    "            f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/\"\n",
    "            f\"datos/fechaini/{fecha_inicio}/fechafin/{fecha_fin}/estacion/{codigo}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            respuesta_meta = requests.get(url_meta, params={\"api_key\": API_KEY})\n",
    "            if respuesta_meta.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            url_datos = respuesta_meta.json().get(\"datos\")\n",
    "            if not url_datos:\n",
    "                continue\n",
    "\n",
    "            respuesta_datos = requests.get(url_datos)\n",
    "            if respuesta_datos.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            datos_json = respuesta_datos.json()\n",
    "            for fila in datos_json:\n",
    "                fila[\"idema\"] = codigo\n",
    "                fila[\"nombre_estacion\"] = nombre\n",
    "                fila[\"timestamp_extraccion\"] = datetime.utcnow().isoformat()\n",
    "                fila[\"id_descarga\"] = id_descarga  # ID de descarga \n",
    "                datos_estacion.append(fila)\n",
    "\n",
    "            time.sleep(1.5)  \n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    todas_las_filas.extend(datos_estacion)\n",
    "\n",
    "# Convertimos a DataFrame y guardamos\n",
    "if todas_las_filas:\n",
    "    df = pd.DataFrame(todas_las_filas)\n",
    "\n",
    "    if os.path.exists(ARCHIVO_SALIDA):\n",
    "        df.to_csv(ARCHIVO_SALIDA, mode=\"a\", index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "\n",
    "    print(\"Los datos se han guardado correctamente.\")\n",
    "else:\n",
    "    print(\"No se obtuvieron datos nuevos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef11a6-2631-461d-a84a-67ca5f595c4b",
   "metadata": {},
   "source": [
    "Parte 2 - Limpieza de Datos\n",
    "\n",
    "Hacer limpieza general de datos\n",
    "\n",
    "Modelar los datos para trabajar cómodamente en una base de datos\n",
    "\n",
    "Ejecutar los scripts de recopilación de datos\n",
    "\n",
    "Considerar aplicar transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae801f-7b06-41d0-9e95-2b4d6b018cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar datos descargados\n",
    "\n",
    "df = pd.read_csv(\"..\\data\\temperaturas_historicas_ampliadas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebbffb-15d7-43be-9f90-8fb9d1fa9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selección de columnas de interés\n",
    "\n",
    "df_I = df[['id_descarga', 'idema', 'nombre_estacion', 'timestamp_estacion', 'provincia', 'altitud', 'fecha', 'tmin', 'tmax', 'tmed', \n",
    "           'hrMedia', 'prec', 'velmedia', 'racha']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaea58-005a-4884-94c6-899195c430f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversión a variables númericas\n",
    "\n",
    "numeric_columns = [\"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"altitud\"]\n",
    "date_columns = [\"fecha\", \"timestamp_extraccion\"]\n",
    "\n",
    "df_I = df_I.copy()\n",
    "\n",
    "for num_col in numeric_columns:\n",
    "    df_I.loc[:, num_col] = pd.to_numeric(df_I[num_col].astype(str).str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
    "\n",
    "for date_col in date_columns:\n",
    "    df_I.loc[:, date_col] = pd.to_datetime(df_I[date_col], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0eed2-a2f7-4370-8905-29dd4170085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizar la distribución de las variables \n",
    "\n",
    "numeric_cols = df_II.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "df_II[numeric_cols].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#No se realizarán transformaciones. \n",
    "#Las variables críticas (tmed, tmin, tmax) para la predicción de la temperatura media tienen una distribución aparentemente normal.\n",
    "#Las variables no críticas transformadas pueden dificultar la interpretación de los gráficos históricos en Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd8357-e06d-4f78-951b-5c5a2ccda68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconocer la cantidad de NA por variable\n",
    "\n",
    "missing_values_count = df_II.isnull().sum()\n",
    "\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a270dfd-17e2-4358-a3cd-322878454e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluar los días sin registro por estación\n",
    "\n",
    "def max_consecutive_nans(series):\n",
    "    is_nan = series.isna()\n",
    "    # Count runs of consecutive NaNs\n",
    "    return is_nan.groupby((~is_nan).cumsum()).transform('size').where(is_nan, 0).max()\n",
    "\n",
    "numeric_cols = df_II.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"Max consecutive NaNs in {col}:\")\n",
    "    max_gaps = df_II.groupby(\"idema\")[col].apply(max_consecutive_nans)\n",
    "    print(max_gaps.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcd14f-90c7-4c35-b5c1-3ce4943d1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamiento de NAs\n",
    "#Premisas\n",
    "\n",
    "#A. Preservar la distribución de las variables\n",
    "#B. Preservar la estructura temporal y espacial de los datos\n",
    "#C. No reemplazar valores fuera del periodo de actividad de las estaciones\n",
    "\n",
    "def smart_impute_respecting_operation(df, numeric_cols, gap_threshold=3): # gap_threshold es el limite de días seguidos sin Na para reemplazar por la mediana\n",
    "    df = df.sort_values([\"idema\", \"fecha\"]).copy() #Ordenar por estación y fecha para poder interpolar linealmente\n",
    "\n",
    "    station_ranges = df.groupby(\"idema\")[\"fecha\"].agg([\"min\", \"max\"]) #Definir el periodo de actividad de cada estación\n",
    "\n",
    "    def fill_gaps(group):\n",
    "        start, end = station_ranges.loc[group.name]\n",
    "        group = group[(group[\"fecha\"] >= start) & (group[\"fecha\"] <= end)].copy() \n",
    "\n",
    "        for col in numeric_cols:\n",
    "            series = group[col]\n",
    "            median_val = series.median() #Calcular la mediana para cada variable numérica\n",
    "\n",
    "            is_nan = series.isna() #Serie booleana de Nas\n",
    "            groups = (is_nan != is_nan.shift()).cumsum() #Identificar los gaps (bloque continuo de NAs) con shift y cada gap recibe un número de grupo único\n",
    "\n",
    "            for g in groups[is_nan].unique():\n",
    "                gap_idx = groups[groups == g].index\n",
    "                gap_len = len(gap_idx) #Cantidad de días consecutivoss con NAs\n",
    "\n",
    "                if gap_len <= gap_threshold:\n",
    "                    group.loc[gap_idx, col] = median_val #Si el periodo consecutivo con NAs (gap) es menor o igual al gap_threshold en el periodo de actividad, entonces replazar con mediana\n",
    "\n",
    "            group[col] = group[col].interpolate(method=\"linear\", limit_direction=\"both\") #Imputar linealmente en gaps mayores al gap_threshold\n",
    "            group[col] = group[col].fillna(median_val) #Si quedó algún gap, entonces completar con la mediana\n",
    "\n",
    "        return group\n",
    "\n",
    "    df_imputed = df.groupby(\"idema\").apply(fill_gaps) #Agrupar por estación y reemplazar Na\n",
    "    df_imputed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Descartar filas que aun preserven valores NA para el target\n",
    "    df_imputed = df_imputed.dropna(subset=['tmed']).reset_index(drop=True)\n",
    "\n",
    "    # Reemplazar con valor '0' los Na en la variable precipitación\n",
    "    if 'prec' in df_imputed.columns:\n",
    "        df_imputed['prec'] = df_imputed['prec'].fillna(0)\n",
    "\n",
    "    # Incorporar ID único de limpieza\n",
    "    df_imputed[\"id_limpieza\"] = range(len(df_imputed))\n",
    "\n",
    "    # Ordenar output\n",
    "    ordered_columns = [\n",
    "        'id_descarga', 'id_limpieza' 'idema', 'nombre_estación', 'timestamp_estacion',\n",
    "        'provincia', 'altitud', 'fecha', 'tmin', 'tmax', 'tmed', \n",
    "        'hrMedia', 'prec', 'velmedia', 'racha'\n",
    "    ]\n",
    "    final_columns = [col for col in ordered_columns if col in df_imputed.columns]\n",
    "\n",
    "    return df_imputed[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f732e6b-561f-4d20-bac2-172e6c762d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar tratamiento\n",
    "\n",
    "numeric_cols = [\"tmin\", \"tmax\", \"tmed\", \"hrMedia\", \"prec\", \"velmedia\", \"racha\"]\n",
    "\n",
    "df_III = smart_impute_respecting_operation(df_II, numeric_cols, gap_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027dc43-13f5-4a1f-8c2a-e8c610fcfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check distribución de las variables\n",
    "\n",
    "numeric_cols = df_III.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "df_III[numeric_cols].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95111e2b-9cda-4069-9c69-7346e4640f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check tratamiento de NA\n",
    "\n",
    "missing_values_count = df_III.isnull().sum()\n",
    "\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31961270-d176-43f1-a351-7ef9eb7dad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar\n",
    "\n",
    "ruta_salida = \"..\\data\\temperaturas_limpias.csv\"\n",
    "df_III.to_csv(ruta_salida, index=False)\n",
    "print(\":marca_de_verificación_blanca: Datos limpios guardados en:\", ruta_salida)\n",
    "print(\":gráfico_de_barras: Dimensiones finales:\", df_III.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
