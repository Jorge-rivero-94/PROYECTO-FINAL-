{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c82c90d",
   "metadata": {},
   "source": [
    "# Predicciones Meteorológicas (AEMET) - SPRINT I\n",
    "\n",
    "Parte 1 - Extracción de Datos\n",
    "\n",
    "Navegar la documentación de la API de AEMET y explorar los endpoints\n",
    "\n",
    "Desarrollar un script que extraiga la información histórica de todas las provincias.\n",
    "\n",
    "Ejecutar el script para extraer los datos de los últimos dos años y verificar que todo funcione correctamente.\n",
    "\n",
    "En el modelo de datos, cada registro debe tener un timestamp de extracción y un identificador para que se pueda manejar el sistema de actualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"AEMET_API_KEY\")\n",
    "CSV_ESTACIONES = \"data/estaciones_filtradas.csv\"\n",
    "ARCHIVO_SALIDA = \"data/temperaturas_historicas_ampliadas.csv\"\n",
    "\n",
    "# 4 rangos de fechas para cubrir 2 años\n",
    "FECHAS = [\n",
    "    (\"2023-05-29T00:00:00UTC\", \"2023-11-28T00:00:00UTC\"),\n",
    "    (\"2023-11-29T00:00:00UTC\", \"2024-05-28T00:00:00UTC\"),\n",
    "    (\"2024-05-29T00:00:00UTC\", \"2024-11-28T00:00:00UTC\"),\n",
    "    (\"2024-11-29T00:00:00UTC\", \"2025-05-28T00:00:00UTC\")\n",
    "]\n",
    "\n",
    "# Cargamos los idema descargados\n",
    "if os.path.exists(ARCHIVO_SALIDA):\n",
    "    datos_existentes = pd.read_csv(ARCHIVO_SALIDA)\n",
    "    estaciones_descargadas = set(datos_existentes[\"idema\"].unique())\n",
    "else:\n",
    "    estaciones_descargadas = set()\n",
    "\n",
    "# Estaciones\n",
    "estaciones = pd.read_csv(CSV_ESTACIONES)\n",
    "\n",
    "# Lista para guardar todos los datos\n",
    "todas_las_filas = []\n",
    "\n",
    "# Creamos identificador único para esta descarga\n",
    "id_descarga = str(uuid.uuid4())\n",
    "\n",
    "print(\"Cargando estaciones...\")\n",
    "\n",
    "for _, fila in estaciones.iterrows():\n",
    "    codigo = fila[\"indicativo\"]\n",
    "    nombre = fila[\"nombre\"]\n",
    "\n",
    "    if codigo in estaciones_descargadas:\n",
    "        continue\n",
    "\n",
    "    print(f\"Procesando estación: {codigo} - {nombre}\")\n",
    "\n",
    "    datos_estacion = []\n",
    "    for fecha_inicio, fecha_fin in FECHAS:\n",
    "        url_meta = (\n",
    "            f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/\"\n",
    "            f\"datos/fechaini/{fecha_inicio}/fechafin/{fecha_fin}/estacion/{codigo}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            respuesta_meta = requests.get(url_meta, params={\"api_key\": API_KEY})\n",
    "            if respuesta_meta.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            url_datos = respuesta_meta.json().get(\"datos\")\n",
    "            if not url_datos:\n",
    "                continue\n",
    "\n",
    "            respuesta_datos = requests.get(url_datos)\n",
    "            if respuesta_datos.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            datos_json = respuesta_datos.json()\n",
    "            for fila in datos_json:\n",
    "                fila[\"idema\"] = codigo\n",
    "                fila[\"nombre_estacion\"] = nombre\n",
    "                fila[\"timestamp_extraccion\"] = datetime.utcnow().isoformat()\n",
    "                fila[\"id_descarga\"] = id_descarga  # ID de descarga \n",
    "                datos_estacion.append(fila)\n",
    "\n",
    "            time.sleep(1.5)  \n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    todas_las_filas.extend(datos_estacion)\n",
    "\n",
    "# Convertimos a DataFrame y guardamos\n",
    "if todas_las_filas:\n",
    "    df = pd.DataFrame(todas_las_filas)\n",
    "\n",
    "    if os.path.exists(ARCHIVO_SALIDA):\n",
    "        df.to_csv(ARCHIVO_SALIDA, mode=\"a\", index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "\n",
    "    print(\"Los datos se han guardado correctamente.\")\n",
    "else:\n",
    "    print(\"No se obtuvieron datos nuevos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef11a6-2631-461d-a84a-67ca5f595c4b",
   "metadata": {},
   "source": [
    "Parte 2 - Limpieza de Datos\n",
    "\n",
    "Hacer limpieza general de datos (Selección de columnas de interés, Tratamiento de NAs, Considerar Encoding)\n",
    "\n",
    "Modelar los datos para trabajar cómodamente en una base de datos (Consistencia en el nombre de las columnas)\n",
    "\n",
    "Ejecutar los scripts de recopilación de datos\n",
    "\n",
    "Considerar aplicar transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cargamos \n",
    "ruta_entrada = \"/data/temperaturas_historicas_ampliadas.csv\"\n",
    "if not os.path.exists(ruta_entrada):\n",
    "    raise FileNotFoundError(f\"No encontré archivo '{ruta_entrada}'. Revisa la ruta.\")\n",
    "\n",
    "df = pd.read_csv(ruta_entrada, dtype=str)\n",
    "\n",
    "# Convertimos columnas numéricas y de fecha al tipo apropiado\n",
    "columnas_numéricas = [\n",
    "    \"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\", \"altitud\"\n",
    "]\n",
    "for col in columnas_numéricas:\n",
    "    df[col] = pd.to_numeric(\n",
    "        df[col].astype(str).str.replace(\",\", \".\", regex=False),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "df[\"timestamp_extraccion\"] = pd.to_datetime(df[\"timestamp_extraccion\"], errors=\"coerce\")\n",
    "\n",
    "# Selecciono solo las columnas que necesito\n",
    "df = df[[\n",
    "    \"id_descarga\",\n",
    "    \"indicativo\",\n",
    "    \"nombre\",\n",
    "    \"provincia\",\n",
    "    \"altitud\",\n",
    "    \"fecha\",\n",
    "    \"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\",\n",
    "    \"timestamp_extraccion\"\n",
    "]].copy()\n",
    "\n",
    "# Estandarizar nombres \n",
    "mapa_provincias = {\n",
    "    'STA. CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'SANTA CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'ILLES BALEARS': 'Illes Balears',\n",
    "    'BALEARES': 'Illes Balears',\n",
    "    'A CORUÑA': 'A Coruña',\n",
    "    'GIRONA': 'Girona',\n",
    "    'LAS PALMAS': 'Las Palmas',\n",
    "    'PONTEVEDRA': 'Pontevedra',\n",
    "    'CANTABRIA': 'Cantabria',\n",
    "    'MALAGA': 'Málaga',\n",
    "    'ALMERIA': 'Almería',\n",
    "    'MURCIA': 'Murcia',\n",
    "    'ALBACETE': 'Albacete',\n",
    "    'AVILA': 'Ávila',\n",
    "    'ARABA/ALAVA': 'Araba/Álava',\n",
    "    'BADAJOZ': 'Badajoz',\n",
    "    'ALICANTE': 'Alacant/Alicante',\n",
    "    'CASTELLON': 'Castelló/Castellón',\n",
    "    'OURENSE': 'Ourense',\n",
    "    'BARCELONA': 'Barcelona',\n",
    "    'BURGOS': 'Burgos',\n",
    "    'CACERES': 'Cáceres',\n",
    "    'CADIZ': 'Cádiz',\n",
    "    'CIUDAD REAL': 'Ciudad Real',\n",
    "    'JAEN': 'Jaén',\n",
    "    'CORDOBA': 'Córdoba',\n",
    "    'CUENCA': 'Cuenca',\n",
    "    'GRANADA': 'Granada',\n",
    "    'GUADALAJARA': 'Guadalajara',\n",
    "    'GIPUZKOA': 'Gipuzkoa/Guipúzcoa',\n",
    "    'HUESCA': 'Huesca',\n",
    "    'LEON': 'León',\n",
    "    'LLEIDA': 'Lleida',\n",
    "    'LA RIOJA': 'La Rioja',\n",
    "    'SORIA': 'Soria',\n",
    "    'NAVARRA': 'Navarra',\n",
    "    'CEUTA': 'Ceuta',\n",
    "    'LUGO': 'Lugo',\n",
    "    'MADRID': 'Madrid',\n",
    "    'PALENCIA': 'Palencia',\n",
    "    'SALAMANCA': 'Salamanca',\n",
    "    'SEGOVIA': 'Segovia',\n",
    "    'SEVILLA': 'Sevilla',\n",
    "    'TOLEDO': 'Toledo',\n",
    "    'TARRAGONA': 'Tarragona',\n",
    "    'TERUEL': 'Teruel',\n",
    "    'VALENCIA': 'València/Valencia',\n",
    "    'VALLADOLID': 'Valladolid',\n",
    "    'BIZKAIA': 'Bizkaia/Vizcaya',\n",
    "    'ZAMORA': 'Zamora',\n",
    "    'ZARAGOZA': 'Zaragoza',\n",
    "    'MELILLA': 'Melilla',\n",
    "    'ASTURIAS': 'Asturias',\n",
    "    'HUELVA': 'Huelva'\n",
    "}\n",
    "\n",
    "df[\"provincia\"] = df[\"provincia\"].map(mapa_provincias).fillna(df[\"provincia\"])\n",
    "\n",
    "# Ordeno por estación y fecha\n",
    "df = df.sort_values([\"indicativo\", \"fecha\"]).reset_index(drop=True)\n",
    "\n",
    "# Eliminamos valores que no convienen\n",
    "df.loc[(df[\"tmin\"] > 45) | (df[\"tmin\"] < -25), \"tmin\"] = pd.NA\n",
    "df.loc[(df[\"tmax\"] > 50) | (df[\"tmax\"] < -25), \"tmax\"] = pd.NA\n",
    "df.loc[(df[\"tmed\"] > 45) | (df[\"tmed\"] < -20), \"tmed\"] = pd.NA\n",
    "df.loc[df[\"prec\"] > 300, \"prec\"] = pd.NA\n",
    "df.loc[df[\"velmedia\"] > 25, \"velmedia\"] = pd.NA\n",
    "df.loc[df[\"racha\"] > 50, \"racha\"] = pd.NA\n",
    "df.loc[(df[\"hrMedia\"] < 5) | (df[\"hrMedia\"] > 100), \"hrMedia\"] = pd.NA\n",
    "\n",
    "# Coherencia lógica: tmin ≤ tmax\n",
    "df.loc[df[\"tmin\"] > df[\"tmax\"], [\"tmin\", \"tmax\", \"tmed\"]] = pd.NA\n",
    "\n",
    "# Hago función para rellenar huecos con la mediana e interpolar el resto\n",
    "def rellenar_por_estacion(grupo):\n",
    "    inicio, fin = grupo[\"fecha\"].min(), grupo[\"fecha\"].max()\n",
    "    grupo = grupo[(grupo[\"fecha\"] >= inicio) & (grupo[\"fecha\"] <= fin)].copy()\n",
    "\n",
    "    columnas_a_imputar = [\"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\"]\n",
    "    for col in columnas_a_imputar:\n",
    "        serie = grupo[col]\n",
    "        mediana = serie.median()\n",
    "        es_nan = serie.isna()\n",
    "        # Bloques de NaN seguidos\n",
    "        bloques = (es_nan != es_nan.shift()).cumsum()\n",
    "        for b in bloques[es_nan].unique():\n",
    "            idx_bloque = bloques[bloques == b].index\n",
    "            if len(idx_bloque) <= 3:\n",
    "                grupo.loc[idx_bloque, col] = mediana\n",
    "        # Interpolamos linealmente donde queden NaN\n",
    "        grupo[col] = grupo[col].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        # Si aún hay NaN, completo con la mediana\n",
    "        grupo[col] = grupo[col].fillna(mediana)\n",
    "\n",
    "    return grupo\n",
    "\n",
    "# Tratamiento de Na a cada estación\n",
    "df = df.groupby(\"indicativo\", group_keys=False).apply(rellenar_por_estacion).reset_index(drop=True)\n",
    "\n",
    "# Elimino filas que todavía tengan tmed vacío\n",
    "df = df.dropna(subset=[\"tmed\"]).reset_index(drop=True)\n",
    "\n",
    "# En lluvias convertimos NaN a 0\n",
    "df[\"prec\"] = df[\"prec\"].fillna(0)\n",
    "\n",
    "# Agregamos un identificador de limpieza \n",
    "df[\"id_limpieza\"] = range(1, len(df) + 1)\n",
    "\n",
    "# Guardamos el CSV limpio\n",
    "ruta_salida = \"data/temperaturas_limpias.csv\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(\"terminamos de limpiar los datos.\")\n",
    "print(\"El CSV limpio se guardó en:\", ruta_salida)\n",
    "print(\"Tamaño final:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a24304",
   "metadata": {},
   "source": [
    "Adicional construcción NUEVO_temperaturas_limpias_outliers para facilitar EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar CSV\n",
    "df = pd.read_csv(\"data/temperaturas_historicas_todas.csv\", dtype=str)\n",
    "\n",
    "# Seleccionar solo las columnas de interés\n",
    "df = df[[\n",
    "    \"id_descarga\",\n",
    "    \"indicativo\",\n",
    "    \"nombre\",\n",
    "    \"provincia\",\n",
    "    \"altitud\",\n",
    "    \"fecha\",\n",
    "    \"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\",\n",
    "    \"timestamp_extraccion\"\n",
    "]]\n",
    "\n",
    "# Convertir columnas numéricas\n",
    "num_cols = [\"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\", \"altitud\"]\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col].str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# Convertir fechas\n",
    "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\")\n",
    "df[\"timestamp_extraccion\"] = pd.to_datetime(df[\"timestamp_extraccion\"], errors=\"coerce\")\n",
    "\n",
    "# Mapear nombres de provincias\n",
    "mapa_provincia = {\n",
    "    'STA. CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'SANTA CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'ILLES BALEARS': 'Illes Balears',\n",
    "    'BALEARES': 'Illes Balears',\n",
    "    'A CORUÑA': 'A Coruña',\n",
    "    'GIRONA': 'Girona',\n",
    "    'LAS PALMAS': 'Las Palmas',\n",
    "    'PONTEVEDRA': 'Pontevedra',\n",
    "    'CANTABRIA': 'Cantabria',\n",
    "    'MALAGA': 'Málaga',\n",
    "    'ALMERIA': 'Almería',\n",
    "    'MURCIA': 'Murcia',\n",
    "    'ALBACETE': 'Albacete',\n",
    "    'AVILA': 'Ávila',\n",
    "    'ARABA/ALAVA': 'Araba/Álava',\n",
    "    'BADAJOZ': 'Badajoz',\n",
    "    'ALICANTE': 'Alacant/Alicante',\n",
    "    'CASTELLON': 'Castelló/Castellón',\n",
    "    'OURENSE': 'Ourense',\n",
    "    'BARCELONA': 'Barcelona',\n",
    "    'BURGOS': 'Burgos',\n",
    "    'CACERES': 'Cáceres',\n",
    "    'CADIZ': 'Cádiz',\n",
    "    'CIUDAD REAL': 'Ciudad Real',\n",
    "    'JAEN': 'Jaén',\n",
    "    'CORDOBA': 'Córdoba',\n",
    "    'CUENCA': 'Cuenca',\n",
    "    'GRANADA': 'Granada',\n",
    "    'GUADALAJARA': 'Guadalajara',\n",
    "    'GIPUZKOA': 'Gipuzkoa/Guipúzcoa',\n",
    "    'HUESCA': 'Huesca',\n",
    "    'LEON': 'León',\n",
    "    'LLEIDA': 'Lleida',\n",
    "    'LA RIOJA': 'La Rioja',\n",
    "    'SORIA': 'Soria',\n",
    "    'NAVARRA': 'Navarra',\n",
    "    'CEUTA': 'Ceuta',\n",
    "    'LUGO': 'Lugo',\n",
    "    'MADRID': 'Madrid',\n",
    "    'PALENCIA': 'Palencia',\n",
    "    'SALAMANCA': 'Salamanca',\n",
    "    'SEGOVIA': 'Segovia',\n",
    "    'SEVILLA': 'Sevilla',\n",
    "    'TOLEDO': 'Toledo',\n",
    "    'TARRAGONA': 'Tarragona',\n",
    "    'TERUEL': 'Teruel',\n",
    "    'VALENCIA': 'València/Valencia',\n",
    "    'VALLADOLID': 'Valladolid',\n",
    "    'BIZKAIA': 'Bizkaia/Vizcaya',\n",
    "    'ZAMORA': 'Zamora',\n",
    "    'ZARAGOZA': 'Zaragoza',\n",
    "    'MELILLA': 'Melilla',\n",
    "    'ASTURIAS': 'Asturias',\n",
    "    'HUELVA': 'Huelva'\n",
    "}\n",
    "df[\"provincia\"] = df[\"provincia\"].map(mapa_provincia).fillna(df[\"provincia\"])\n",
    "\n",
    "# Ordenar\n",
    "df = df.sort_values([\"indicativo\", \"fecha\"]).reset_index(drop=True)\n",
    "\n",
    "# Reemplazar incoherencias físicas por Na\n",
    "df.loc[(df[\"tmin\"] > 45) | (df[\"tmin\"] < -25), \"tmin\"] = pd.NA\n",
    "df.loc[(df[\"tmax\"] > 50) | (df[\"tmax\"] < -25), \"tmax\"] = pd.NA\n",
    "df.loc[(df[\"tmed\"] > 45) | (df[\"tmed\"] < -20), \"tmed\"] = pd.NA\n",
    "df.loc[df[\"prec\"] > 300, \"prec\"] = pd.NA\n",
    "df.loc[df[\"velmedia\"] > 25, \"velmedia\"] = pd.NA\n",
    "df.loc[df[\"racha\"] > 50, \"racha\"] = pd.NA\n",
    "df.loc[(df[\"hrMedia\"] < 5) | (df[\"hrMedia\"] > 100), \"hrMedia\"] = pd.NA\n",
    "df.loc[df[\"tmin\"] > df[\"tmax\"], [\"tmin\", \"tmax\", \"tmed\"]] = pd.NA\n",
    "\n",
    "# Marcar outliers estadísticos con IQR (Tukey)\n",
    "def marcar_outliers(grupo):\n",
    "    q1 = grupo[\"tmed\"].quantile(0.25)\n",
    "    q3 = grupo[\"tmed\"].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lim_inf = q1 - 1.5 * iqr\n",
    "    lim_sup = q3 + 1.5 * iqr\n",
    "    grupo[\"tmed_outlier\"] = (grupo[\"tmed\"] < lim_inf) | (grupo[\"tmed\"] > lim_sup)\n",
    "    return grupo\n",
    "\n",
    "df = df.groupby(\"indicativo\", group_keys=False).apply(marcar_outliers)\n",
    "\n",
    "# Imputar huecos pequeños (<=3 días) e interpolar\n",
    "def fill_station_gaps(grp):\n",
    "    grp = grp.copy()\n",
    "    for col in [\"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\"]:\n",
    "        serie = grp[col]\n",
    "        mediana = serie.median()\n",
    "        is_nan = serie.isna()\n",
    "        grupos = (is_nan != is_nan.shift()).cumsum()\n",
    "        for g in grupos[is_nan].unique():\n",
    "            idx_gap = grupos[grupos == g].index\n",
    "            if len(idx_gap) <= 3:\n",
    "                grp.loc[idx_gap, col] = mediana\n",
    "        grp[col] = grp[col].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        grp[col] = grp[col].fillna(mediana)\n",
    "    return grp\n",
    "\n",
    "df = df.groupby(\"indicativo\", group_keys=False).apply(fill_station_gaps)\n",
    "\n",
    "# Eliminar filas sin tmed\n",
    "df = df.dropna(subset=[\"tmed\"]).reset_index(drop=True)\n",
    "\n",
    "# 1Precipitación faltante = 0\n",
    "df[\"prec\"] = df[\"prec\"].fillna(0)\n",
    "\n",
    "# ID limpieza\n",
    "df[\"id_limpieza\"] = range(len(df))\n",
    "\n",
    "# Guardar\n",
    "df.to_csv(\"data/temperaturas_limpias_outliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133458bf-7510-4b15-ba7f-245b47278f15",
   "metadata": {},
   "source": [
    "Complemento EDA para Datos_historicos.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18554942-1773-495c-9c96-86115cd45a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# 1) CARGA Y FILTRO: datos de España\n",
    "df = pd.read_csv(\n",
    "    'data/temperaturas_limpias_outliers.csv',\n",
    "    parse_dates=['fecha']\n",
    ")\n",
    "df['provincia'] = df['provincia'].str.strip().str.title()\n",
    "datos = df['tmed'].dropna()\n",
    "\n",
    "# 2) DETECCIÓN DE OUTLIERS (Tukey)\n",
    "q1 = datos.quantile(0.25)\n",
    "q3 = datos.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "limite_bajo = q1 - 1.5 * iqr\n",
    "limite_alto = q3 + 1.5 * iqr\n",
    "normales = datos[(datos >= limite_bajo) & (datos <= limite_alto)]\n",
    "atipicos = datos[(datos < limite_bajo) | (datos > limite_alto)]\n",
    "\n",
    "# 3) BINS PARA HISTOGRAMA\n",
    "puntos = np.linspace(datos.min(), datos.max(), 30)\n",
    "\n",
    "# 4) FIGURA CON 3 SUBGRÁFICOS: HISTOGRAMA, BOXPLOT GENERAL Y BOXPLOT MENSUAL\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1.2])  # 2 filas, 2 columnas\n",
    "\n",
    "# --- Gráficos superiores ---\n",
    "ax_histo = fig.add_subplot(gs[0, 0])  # fila 0, col 0\n",
    "ax_caja = fig.add_subplot(gs[0, 1])   # fila 0, col 1\n",
    "\n",
    "# HISTOGRAMA\n",
    "ax_histo.hist(normales, bins=puntos, alpha=0.7, label='Días normales (Tukey)', color='skyblue')\n",
    "ax_histo.hist(atipicos, bins=puntos, alpha=0.7, label='Días atípicos (Tukey)', color='orange')\n",
    "ax_histo.set_title('Histograma de Tmed 2024 en España')\n",
    "ax_histo.set_xlabel('Temperatura media (°C)')\n",
    "ax_histo.set_ylabel('Cantidad de días')\n",
    "ax_histo.legend()\n",
    "\n",
    "# BOXPLOT GENERAL\n",
    "ax_caja.boxplot(\n",
    "    datos,\n",
    "    vert=True,\n",
    "    showfliers=True,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='skyblue', color='black'),\n",
    "    flierprops=dict(marker='o', markerfacecolor='orange', markersize=5, alpha=0.7)\n",
    ")\n",
    "ax_caja.set_title('Boxplot general de Tmed 2024')\n",
    "ax_caja.set_ylabel('Temperatura media (°C)')\n",
    "\n",
    "# --- Gráfico inferior: BOXPLOT MENSUAL + MEDIAS ---\n",
    "ax_mensual = fig.add_subplot(gs[1, :])  # fila 1, columnas completas\n",
    "\n",
    "# Agregamos columna 'mes'\n",
    "df['mes'] = df['fecha'].dt.month\n",
    "valores_por_mes = [df[df['mes'] == m]['tmed'].dropna() for m in range(1, 13)]\n",
    "medias_por_mes = [mes.mean() if not mes.empty else np.nan for mes in valores_por_mes]\n",
    "\n",
    "# BOXPLOT\n",
    "ax_mensual.boxplot(valores_por_mes, labels=list(range(1, 13)), showfliers=True)\n",
    "\n",
    "# LÍNEA DE TENDENCIA (medias mensuales)\n",
    "ax_mensual.plot(\n",
    "    range(1, 13), medias_por_mes,\n",
    "    marker='o', linestyle='-', color='red', label='Media mensual'\n",
    ")\n",
    "\n",
    "ax_mensual.set_title('Boxplot mensual de temperatura media (España, 2024)')\n",
    "ax_mensual.set_xlabel('Mes')\n",
    "ax_mensual.set_ylabel('Temperatura media (°C)')\n",
    "ax_mensual.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Descriptivos por año\n",
    "desc_2023 = df[df['fecha'].dt.year == 2023]['tmed'].describe()\n",
    "desc_2024 = df[df['fecha'].dt.year == 2024]['tmed'].describe()\n",
    "desc_total = df['tmed'].describe()\n",
    "\n",
    "# Unir en un solo DataFrame\n",
    "resumen = pd.DataFrame({\n",
    "    '2023': desc_2023,\n",
    "    '2024': desc_2024,\n",
    "    '2023-2024': desc_total\n",
    "})\n",
    "\n",
    "# Agregar columna de variación (2024 - 2023)\n",
    "resumen['Variación 2023-2024'] = resumen['2024'] - resumen['2023']\n",
    "\n",
    "# Agrupar por mes y año, calcular media por mes\n",
    "df['año'] = df['fecha'].dt.year\n",
    "estad_2023 = df[df['año'] == 2023].groupby('mes')['tmed'].describe()\n",
    "estad_2024 = df[df['año'] == 2024].groupby('mes')['tmed'].describe()\n",
    "\n",
    "# Unir los DataFrames\n",
    "resumen_mensual = pd.concat([estad_2023.add_suffix('_2023'),\n",
    "                              estad_2024.add_suffix('_2024')],\n",
    "                             axis=1)\n",
    "\n",
    "# Añadir columna de variación de medias\n",
    "resumen_mensual['Variación_media_2024-2023'] = resumen_mensual['mean_2024'] - resumen_mensual['mean_2023']\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# COMPARADOR DE TEMPERATURAS PARA MADRID (2023 vs 2024)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Calculamos para cada fecha: media, mediana, mínimo y máximo de tmed\n",
    "estad = (\n",
    "    df\n",
    "    .groupby('fecha')['tmed']\n",
    "    .agg(['mean', 'median', 'min', 'max'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Separamos en 2023 y en 2024\n",
    "estad_2023 = estad[estad['fecha'].dt.year == 2023]\n",
    "estad_2024 = estad[estad['fecha'].dt.year == 2024]\n",
    "\n",
    "# Preparamos dos subgráficos (uno para cada año), con el mismo eje vertical\n",
    "fig, (eje1, eje2) = plt.subplots(2, 1, figsize=(14, 8), sharey=True)\n",
    "\n",
    "# Gráfico 2023\n",
    "eje1.plot(estad_2023['fecha'], estad_2023['mean'], color='orange', label='Media 2023')\n",
    "eje1.plot(estad_2023['fecha'], estad_2023['median'], color='orange', linestyle='--', label='Mediana 2023')\n",
    "eje1.fill_between(\n",
    "    estad_2023['fecha'],\n",
    "    estad_2023['min'],\n",
    "    estad_2023['max'],\n",
    "    color='orange', alpha=0.1,\n",
    "    label='Rango 2023 (min-max)'\n",
    ")\n",
    "eje1.set_title('España: Temperaturas Diarias 2023', fontsize=14)\n",
    "eje1.set_ylabel('Temperatura (°C)')\n",
    "eje1.legend()\n",
    "\n",
    "# Gráfico 2024\n",
    "eje2.plot(estad_2024['fecha'], estad_2024['mean'], color='blue', label='Media 2024')\n",
    "eje2.plot(estad_2024['fecha'], estad_2024['median'], color='blue', linestyle='--', label='Mediana 2024')\n",
    "eje2.fill_between(\n",
    "    estad_2024['fecha'],\n",
    "    estad_2024['min'],\n",
    "    estad_2024['max'],\n",
    "    color='blue', alpha=0.1,\n",
    "    label='Rango 2024 (min-max)'\n",
    ")\n",
    "eje2.set_title('España: Temperaturas Diarias 2024', fontsize=14)\n",
    "eje2.set_ylabel('Temperatura (°C)')\n",
    "eje2.set_xlabel('Fecha')\n",
    "eje2.legend()\n",
    "\n",
    "# Ajustamos para no montar elementos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d767d",
   "metadata": {},
   "source": [
    "**Conclusiones**\n",
    "\n",
    "En general, 2024 fue más frío que 2023: la media bajó casi 3 °C, como se ve en la fila \"mean\" de la tabla anual.\n",
    "\n",
    "Mes a mes, la mayor caída de temperatura ocurrió en octubre y junio, con casi -2 °C de diferencia respecto a 2023.\n",
    "\n",
    "Noviembre fue la excepción, con temperaturas más altas en 2024 que en 2023, lo que puede sugerir un retraso en el enfriamiento otoñal\n",
    "\n",
    "**Temperatura Media 2024 por Provincia**\n",
    "\n",
    "Min: Castellón 4.5 °C → la media anual más baja.\n",
    "Máx: Las Palmas 21.0 °C → la más cálida del año.\n",
    "\n",
    "**Temperatura Media Agosto 2024 por Provincia**\n",
    "\n",
    "Min: Cantabria 18.0 °C → la más fresca del mes.\n",
    "Máx: Jaén 29.6 °C → la más calurosa.\n",
    "\n",
    "**Temperatura Media 15 de Enero de 2024**\n",
    "\n",
    "Min: Huesca 6.9 °C → la provincia más fría ese día.\n",
    "Máx: Las Palmas 21.5 °C → la más templada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
