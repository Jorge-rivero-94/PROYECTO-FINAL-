{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c82c90d",
   "metadata": {},
   "source": [
    "# Predicciones Meteorol√≥gicas (AEMET) - SPRINT I\n",
    "\n",
    "Parte 1 - Extracci√≥n de Datos\n",
    "\n",
    "Navegar la documentaci√≥n de la API de AEMET y explorar los endpoints\n",
    "\n",
    "Desarrollar un script que extraiga la informaci√≥n hist√≥rica de todas las provincias.\n",
    "\n",
    "Ejecutar el script para extraer los datos de los √∫ltimos dos a√±os y verificar que todo¬†funcione correctamente.\n",
    "\n",
    "En el modelo de datos, cada registro debe tener un timestamp de extracci√≥n y un identificador para que se pueda manejar el sistema de actualizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqb3JnZXJpdmVyb2RlbG9zcmlvc0BnbWFpbC5jb20iLCJqdGkiOiJiMjlhZmM2Zi0yMTkwLTQ4ZTEtYjlmYy01NGY5OTk3OTc1YjUiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTc0ODk2ODY4NSwidXNlcklkIjoiYjI5YWZjNmYtMjE5MC00OGUxLWI5ZmMtNTRmOTk5Nzk3NWI1Iiwicm9sZSI6IiJ9.90idEjGLaI61xKuPe8sdQtBJ2fdf4gwZmsww11V1VpE\"\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"‚ùå No se encontr√≥ AEMET_API_KEY en las variables de entorno.\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Funci√≥n para obtener el inventario completo de estaciones\n",
    "# ---------------------------------------------------\n",
    "def obtener_inventario_completo():\n",
    "    url_inventario = (\n",
    "        \"https://opendata.aemet.es/opendata/api/\"\n",
    "        \"valores/climatologicos/inventarioestaciones/todasestaciones\"\n",
    "    )\n",
    "    r = requests.get(url_inventario, params={\"api_key\": API_KEY}, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    datos_meta = r.json().get(\"datos\")\n",
    "    if not datos_meta:\n",
    "        raise RuntimeError(\"No se obtuvo URL de datos del inventario.\")\n",
    "    r2 = requests.get(datos_meta, timeout=15)\n",
    "    r2.raise_for_status()\n",
    "    estaciones = r2.json()  # Lista de diccionarios\n",
    "    return pd.DataFrame(estaciones)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Funci√≥n para descargar datos diarios para una estaci√≥n\n",
    "# ---------------------------------------------------\n",
    "def descargar_para_una_estacion(idema: str, nombre: str, id_descarga: str, bloques_fechas) -> list:\n",
    "    filas = []\n",
    "    for fecha_ini, fecha_fin in bloques_fechas:\n",
    "        url_meta = (\n",
    "            \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/\"\n",
    "            f\"datos/fechaini/{fecha_ini}/fechafin/{fecha_fin}/estacion/{idema}\"\n",
    "        )\n",
    "        try:\n",
    "            r = requests.get(url_meta, params={\"api_key\": API_KEY}, timeout=15)\n",
    "            if r.status_code != 200:\n",
    "                continue\n",
    "            datos_meta = r.json()\n",
    "            url_real  = datos_meta.get(\"datos\")\n",
    "            if not url_real:\n",
    "                continue\n",
    "\n",
    "            rd = requests.get(url_real, timeout=15)\n",
    "            if rd.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            lista_json = rd.json()\n",
    "            for rec in lista_json:\n",
    "                rec[\"idema\"]               = idema\n",
    "                rec[\"nombre_estacion\"]     = nombre\n",
    "                rec[\"timestamp_extraccion\"]= datetime.utcnow().isoformat()\n",
    "                rec[\"id_descarga\"]         = id_descarga\n",
    "                filas.append(rec)\n",
    "\n",
    "            time.sleep(1.2)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return filas\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Script principal\n",
    "# ---------------------------------------------------\n",
    "def main():\n",
    "    print(\"üì• Iniciando extracci√≥n de datos de TODAS las estaciones...\")\n",
    "\n",
    "    # 1) Obtengo el inventario completo (todas las estaciones)\n",
    "    estaciones_df = obtener_inventario_completo()\n",
    "    estaciones_df = estaciones_df.dropna(subset=[\"indicativo\"])  # descartar filas sin ID v√°lido\n",
    "\n",
    "    # 2) Defino rangos de fechas autom√°ticos de √∫ltimos 2 a√±os, en 4 bloques de ~6 meses\n",
    "    hoy = datetime.utcnow().date()\n",
    "    hace_dos_a√±os = hoy - timedelta(days=730)\n",
    "    bloques = []\n",
    "    inicio = hace_dos_a√±os\n",
    "    while inicio < hoy:\n",
    "        fin = inicio + timedelta(days=182)\n",
    "        if fin > hoy:\n",
    "            fin = hoy\n",
    "        bloques.append((f\"{inicio.isoformat()}T00:00:00UTC\", f\"{fin.isoformat()}T00:00:00UTC\"))\n",
    "        inicio = fin + timedelta(days=1)\n",
    "\n",
    "    # 3) Veo si ya hay un CSV de salida previo, para no re-descargar estaciones\n",
    "    ARCHIVO_SALIDA = \"data/temperaturas_historicas_todas.csv\"\n",
    "    ya_descargadas = set()\n",
    "    if os.path.exists(ARCHIVO_SALIDA):\n",
    "        try:\n",
    "            df_prev = pd.read_csv(ARCHIVO_SALIDA, dtype=str, usecols=[\"idema\"])\n",
    "            ya_descargadas = set(df_prev[\"idema\"].dropna().unique())\n",
    "        except Exception:\n",
    "            ya_descargadas = set()\n",
    "\n",
    "    # 4) Genero un UUID para esta ejecuci√≥n\n",
    "    id_descarga = str(uuid.uuid4())\n",
    "    print(f\"   ‚Ä¢ UUID de esta descarga: {id_descarga}\")\n",
    "    print(f\"   ‚Ä¢ Total de estaciones a procesar: {len(estaciones_df)}\")\n",
    "\n",
    "    # 5) Recorro cada estaci√≥n\n",
    "    todas_las_filas = []\n",
    "    for idx, fila in estaciones_df.iterrows():\n",
    "        idema  = fila[\"indicativo\"]\n",
    "        nombre = fila.get(\"nombre\", \"\")\n",
    "\n",
    "        if idema in ya_descargadas:\n",
    "            print(f\"‚ûñ Saltando {idema} (ya descargada antes)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì° Procesando estaci√≥n: {idema} ‚Äî {nombre} ({idx+1}/{len(estaciones_df)})\")\n",
    "        filas_est = descargar_para_una_estacion(idema, nombre, id_descarga, bloques)\n",
    "        todas_las_filas.extend(filas_est)\n",
    "\n",
    "    # 6) Guardo todo en un CSV final\n",
    "    if todas_las_filas:\n",
    "        df_final = pd.DataFrame(todas_las_filas)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        if os.path.exists(ARCHIVO_SALIDA):\n",
    "            df_final.to_csv(ARCHIVO_SALIDA, mode=\"a\", index=False, header=False)\n",
    "        else:\n",
    "            df_final.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "        print(f\"‚úÖ Extracci√≥n completada. Guardado: '{ARCHIVO_SALIDA}'\")\n",
    "        print(f\"   ‚Üí Filas nuevas: {len(df_final)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se obtuvieron datos nuevos en esta ejecuci√≥n.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef11a6-2631-461d-a84a-67ca5f595c4b",
   "metadata": {},
   "source": [
    "Parte 2 - Limpieza de Datos\n",
    "\n",
    "Hacer limpieza general de datos\n",
    "\n",
    "Modelar los datos para trabajar c√≥modamente en una base de datos\n",
    "\n",
    "Ejecutar los scripts de recopilaci√≥n de datos\n",
    "\n",
    "Considerar aplicar transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1) Cargamos \n",
    "ruta_entrada = r\"C:\\Users\\User\\OneDrive - Universidade de Santiago de Compostela\\Documentos\\Data Science\\temperaturas_historicas_todas.csv\"\n",
    "if not os.path.exists(ruta_entrada):\n",
    "    raise FileNotFoundError(f\"No encontr√© archivo '{ruta_entrada}'. Revisa la ruta.\")\n",
    "\n",
    "df = pd.read_csv(ruta_entrada, dtype=str)\n",
    "\n",
    "# 2) Convertimos columnas num√©ricas y de fecha al tipo apropiado\n",
    "columnas_num√©ricas = [\n",
    "    \"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\", \"altitud\"\n",
    "]\n",
    "for col in columnas_num√©ricas:\n",
    "    df[col] = pd.to_numeric(\n",
    "        df[col].astype(str).str.replace(\",\", \".\", regex=False),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "df[\"timestamp_extraccion\"] = pd.to_datetime(df[\"timestamp_extraccion\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Selecciono solo las columnas que necesito\n",
    "df = df[[\n",
    "    \"id_descarga\",\n",
    "    \"indicativo\",\n",
    "    \"nombre\",\n",
    "    \"provincia\",\n",
    "    \"altitud\",\n",
    "    \"fecha\",\n",
    "    \"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\",\n",
    "    \"timestamp_extraccion\"\n",
    "]].copy()\n",
    "\n",
    "# 4) Estandarizar nombres \n",
    "provincia_map = {\n",
    "    'STA. CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'SANTA CRUZ DE TENERIFE': 'Santa Cruz De Tenerife',\n",
    "    'ILLES BALEARS': 'Illes Balears',\n",
    "    'BALEARES': 'Illes Balears',\n",
    "    'A CORU√ëA': 'A Coru√±a',\n",
    "    'GIRONA': 'Girona',\n",
    "    'LAS PALMAS': 'Las Palmas',\n",
    "    'PONTEVEDRA': 'Pontevedra',\n",
    "    'CANTABRIA': 'Cantabria',\n",
    "    'MALAGA': 'M√°laga',\n",
    "    'ALMERIA': 'Almer√≠a',\n",
    "    'MURCIA': 'Murcia',\n",
    "    'ALBACETE': 'Albacete',\n",
    "    'AVILA': '√Åvila',\n",
    "    'ARABA/ALAVA': 'Araba/√Ålava',\n",
    "    'BADAJOZ': 'Badajoz',\n",
    "    'ALICANTE': 'Alacant/Alicante',\n",
    "    'CASTELLON': 'Castell√≥/Castell√≥n',\n",
    "    'OURENSE': 'Ourense',\n",
    "    'BARCELONA': 'Barcelona',\n",
    "    'BURGOS': 'Burgos',\n",
    "    'CACERES': 'C√°ceres',\n",
    "    'CADIZ': 'C√°diz',\n",
    "    'CIUDAD REAL': 'Ciudad Real',\n",
    "    'JAEN': 'Ja√©n',\n",
    "    'CORDOBA': 'C√≥rdoba',\n",
    "    'CUENCA': 'Cuenca',\n",
    "    'GRANADA': 'Granada',\n",
    "    'GUADALAJARA': 'Guadalajara',\n",
    "    'GIPUZKOA': 'Gipuzkoa/Guip√∫zcoa',\n",
    "    'HUESCA': 'Huesca',\n",
    "    'LEON': 'Le√≥n',\n",
    "    'LLEIDA': 'Lleida',\n",
    "    'LA RIOJA': 'La Rioja',\n",
    "    'SORIA': 'Soria',\n",
    "    'NAVARRA': 'Navarra',\n",
    "    'CEUTA': 'Ceuta',\n",
    "    'LUGO': 'Lugo',\n",
    "    'MADRID': 'Madrid',\n",
    "    'PALENCIA': 'Palencia',\n",
    "    'SALAMANCA': 'Salamanca',\n",
    "    'SEGOVIA': 'Segovia',\n",
    "    'SEVILLA': 'Sevilla',\n",
    "    'TOLEDO': 'Toledo',\n",
    "    'TARRAGONA': 'Tarragona',\n",
    "    'TERUEL': 'Teruel',\n",
    "    'VALENCIA': 'Val√®ncia/Valencia',\n",
    "    'VALLADOLID': 'Valladolid',\n",
    "    'BIZKAIA': 'Bizkaia/Vizcaya',\n",
    "    'ZAMORA': 'Zamora',\n",
    "    'ZARAGOZA': 'Zaragoza',\n",
    "    'MELILLA': 'Melilla',\n",
    "    'ASTURIAS': 'Asturias',\n",
    "    'HUELVA': 'Huelva'\n",
    "}\n",
    "\n",
    "df[\"provincia\"] = df[\"provincia\"].map(mapa_provincias).fillna(df[\"provincia\"])\n",
    "\n",
    "# 5) Ordeno por estaci√≥n y fecha\n",
    "df = df.sort_values([\"indicativo\", \"fecha\"]).reset_index(drop=True)\n",
    "\n",
    "# 6) Eliminamos valores que no convienen\n",
    "df.loc[(df[\"tmin\"] > 45) | (df[\"tmin\"] < -25), \"tmin\"] = pd.NA\n",
    "df.loc[(df[\"tmax\"] > 50) | (df[\"tmax\"] < -25), \"tmax\"] = pd.NA\n",
    "df.loc[(df[\"tmed\"] > 45) | (df[\"tmed\"] < -20), \"tmed\"] = pd.NA\n",
    "df.loc[df[\"prec\"] > 300, \"prec\"] = pd.NA\n",
    "df.loc[df[\"velmedia\"] > 25, \"velmedia\"] = pd.NA\n",
    "df.loc[df[\"racha\"] > 50, \"racha\"] = pd.NA\n",
    "df.loc[(df[\"hrMedia\"] < 5) | (df[\"hrMedia\"] > 100), \"hrMedia\"] = pd.NA\n",
    "\n",
    "# 6. b) coherencia l√≥gica: tmin ‚â§ tmax\n",
    "df.loc[df[\"tmin\"] > df[\"tmax\"], [\"tmin\", \"tmax\", \"tmed\"]] = pd.NA\n",
    "\n",
    "# 7) Hago funci√≥n para rellenar huecos con la mediana e interpolar el resto\n",
    "def rellenar_por_estacion(grupo):\n",
    "    inicio, fin = grupo[\"fecha\"].min(), grupo[\"fecha\"].max()\n",
    "    grupo = grupo[(grupo[\"fecha\"] >= inicio) & (grupo[\"fecha\"] <= fin)].copy()\n",
    "\n",
    "    columnas_a_imputar = [\"tmin\", \"tmax\", \"tmed\", \"prec\", \"velmedia\", \"racha\", \"hrMedia\"]\n",
    "    for col in columnas_a_imputar:\n",
    "        serie = grupo[col]\n",
    "        mediana = serie.median()\n",
    "        es_nan = serie.isna()\n",
    "        # Bloques de NaN seguidos\n",
    "        bloques = (es_nan != es_nan.shift()).cumsum()\n",
    "        for b in bloques[es_nan].unique():\n",
    "            idx_bloque = bloques[bloques == b].index\n",
    "            if len(idx_bloque) <= 3:\n",
    "                grupo.loc[idx_bloque, col] = mediana\n",
    "        # Interpolamos linealmente donde queden NaN\n",
    "        grupo[col] = grupo[col].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        # Si a√∫n hay NaN, completo con la mediana\n",
    "        grupo[col] = grupo[col].fillna(mediana)\n",
    "\n",
    "    return grupo\n",
    "\n",
    "# 8) Su funci√≥n a cada estaci√≥n\n",
    "df = df.groupby(\"indicativo\", group_keys=False).apply(rellenar_por_estacion).reset_index(drop=True)\n",
    "\n",
    "# 9) Elimino filas que todav√≠a tengan tmed vac√≠o\n",
    "df = df.dropna(subset=[\"tmed\"]).reset_index(drop=True)\n",
    "\n",
    "# 10) En lluvias convertimos NaN a 0\n",
    "df[\"prec\"] = df[\"prec\"].fillna(0)\n",
    "\n",
    "# 11) Agregamos un identificador de limpieza \n",
    "df[\"id_limpieza\"] = range(1, len(df) + 1)\n",
    "\n",
    "# 12) Guardamos el CSV limpio\n",
    "ruta_salida = \"data/temperaturas_limpias.csv\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(\"terminamos de limpiar los datos.\")\n",
    "print(\"El CSV limpio se guard√≥ en:\", ruta_salida)\n",
    "print(\"Tama√±o final:\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
